apiVersion: "nais.io/v1alpha1"
kind: "Alert"
metadata:
  name: harbeidsgiver-alerts
  labels:
    team: helsearbeidsgiver
spec:
  receivers:
    slack:
      channel: '#team-hell-alerts'
      #prependText: '<!here> | '
  alerts:
    - alert: harbeidsgiver-app-nede
      expr: up{app=~"syfoinntektsmelding|spberegning|helse-spion|helse-spion-frontend",job="kubernetes-pods"} == 0
      for: 2m
      description: "{{ $labels.app }} er nede"
      action: "Se `kubectl describe pod {{ $labels.kubernetes_pod_name }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }}` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: harebeidsgiver-kontinuerlig-restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"syfoinntektsmelding|spberegning|helse-spion|helse-spion-frontend"}[30m])) by (container) > 2
      for: 5m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen!"
      action: "Se `kubectl describe pod {{ $labels.container }}` for events, og `kubectl logs {{ $labels.container }}` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: harebeidsgiver-mangler-metrikker
      expr: absent(up{app=~"syfoinntektsmelding|spberegning",job="kubernetes-pods"})
      for: 5m
      description: "{{ $labels.app }} rapporterer ingen metrikker"
      action: "Sjekk om {{ $labels.app }} er oppe"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: harebeidsgiver-errorlogging
      expr: (100 * sum by (log_app) (rate(logd_messages_total{log_app=~"syfoinntektsmelding|spberegning|helse-spion|helse-spion-frontend",log_level="Error"}[5m])) / sum by (log_app) (rate(logd_messages_total{log_app=~"syfopdfgen|syfoarbeidsgivertilgang|syfogsak|syfonarmesteleder|syfoservicestrangler|syfosoknadvarsel|syfosoknad|syfoaltinn"}[5m]))) > 10
      for: 3m
      description: "{{ $labels.log_app }} rapporterer error i loggene"
      action: "Sjekk loggene til {{ $labels.log_app }}, for å se hvorfor det er så mye feil (over 10 feil per 100 logger de siste 5 minuttene)"
      #App-spesifikke alerts
      - alert: syfogsak-ingenSoknaderBehandlet
        expr: sum(increase(syfogsak_innsending_behandlet_total[24h])) < 0.8
        for: 1m
        description: "{{ $labels.app }} har ikke behandlet soknader på 24 timer"
        action: "Syfogsak har ikke behandlet soknader på 24 timer. Sjekk i errorloggen: https://logs.adeo.no/goto/c117ef2a4003c420e43d391f8007ddda , og evt. `kubectl delete pod {{ $labels.kubernetes_pod_name }}` for å restarte"
        sla: respond within 1h, during office hours
        severity: danger